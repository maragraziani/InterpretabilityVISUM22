{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Demo_to_share.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wTcP6poyCx-l",
        "Ohvv6lw41u_s",
        "TjUOQ8bwHI86",
        "MRilKvlbKgxv",
        "zX2b0zyHIk2B",
        "v6xPA24bN3R7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maragraziani/InterpretabilityVISUM22/blob/main/Uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISUM 2022 - Exaplainable AI - Hands on session**\n",
        "# Uncertainty in white matter lesion segmentation\n",
        "**written by Vatsal Raina**\n",
        "* Ph.D. candidate at University of Cambridge\n",
        "* Work performed during his internship at Hes-so Valais\n",
        "* vr311@cam.ac.uk \n",
        "\n",
        "**led by Mara Graziani**\n",
        "* postdoctoral researcher at Hes-so Valais and IBM Research x ZHAW\n",
        "* mara.graziani@hevs.ch ; @mormontre\n",
        "\n",
        "## Content\n",
        "\n",
        "1. Motivation \n",
        "2. Uncertainty Estimation - basics\n",
        "3. Demo\n",
        "\n",
        "## Take Aways\n",
        "\n",
        "*   Total Uncertainty = Data Uncertainty + Model Uncertainty\n",
        "*   Simplest estimate of total uncertainty is the Entropy of the expected outcome (computed over model ensembles)\n",
        "*   Model uncertainty can be estimated through mutual information \n",
        "\n",
        "### Acknowledgements and References\n",
        "\n",
        "* Malinin, Andrey et al. “Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks.” *ArXiv abs/2107.07455 (2021)*\n",
        "* Koh, Pang Wei, et al. \"Wilds: A benchmark of in-the-wild distribution shifts.\" International Conference on Machine Learning. PMLR, 2021.\n",
        "* Nair, Tanya, et al. \"Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation.\"* Medical image analysis 59 (2020): 101557.*\n",
        "\n",
        "### Explore further\n",
        "\n",
        "*   Join the Shifts Challenge!\n",
        "*  https://oatml.cs.ox.ac.uk/blog/2021/08/06/shifts.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3zBqR_rotA83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Motivation\n",
        "\n",
        "*   Deep Learning models show poor generalization to real world datasets [1], little robustness to adversarial examples [2] and dangerous hidden biases [3]\n",
        "*   In general, the greater the dataset shift between training and deployment data, the poorer the model performance on the unseen dataset. \n",
        "* Evaluating the model in terms of the sole performance on in-domain test data is not sufficient [4] \n",
        "* Estimating model uncertainty gives further insights on the model performance and it can lead to more actionable decisions in human-machine pairings.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1]Arvidsson, I., Overgaard, N. C., Marginean, F.-E., Krzyzanowska, A., Bjartell, A., ̊Astr ̈om, K. & Heyden, A. (2018), Generalization of prostate cancer classification for multiple sites using deep learning, in ‘2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)’, IEEE, pp. 191–194. \n",
        "\n",
        "[2]Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n",
        "\n",
        "[3]Zou, James, and Londa Schiebinger. \"AI can be sexist and racist—it’s time to make it fair.\" Nature (2018): 324-326.\n",
        "\n",
        "[4]Doshi-Velez, Finale, and Been Kim. \"Towards a rigorous science of interpretable machine learning.\" arXiv preprint arXiv:1702.08608 (2017).\n",
        "\n"
      ],
      "metadata": {
        "id": "OIDBEGckCsRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Uncertainty Estimation Basics\n",
        "\n",
        "Consider the following example (credits: TwinEd Productions https://www.youtube.com/watch?v=Osju20L6Z3I)\n",
        "\n",
        "![picture](https://github.com/maragraziani/InterpretabilityVISUM22/blob/main/images/basics_uncertainty.png?raw=true)\n",
        "\n",
        "In the first case, the model has high certainty about the prediction. \n",
        "\n",
        "In the second case, the distribution on the output classes is quite uniform, and this characterizes a high uncertainty of the model on the predicted class. \n",
        "\n",
        "When we have a single model, we can only estimate data uncertainty.\n",
        "\n",
        "#### A common way to estimate total uncertainty is through model ensembles. \n",
        "\n",
        "* Multiple models with identical architectures are trained with different initialization seeds on the same training data and tested on the same image. \n",
        "* For N models in the ensemble, we obtain N different softmax outcomes\n",
        "\n",
        "#### From the ensemble, we can compute multiple uncertainty estimates\n",
        "\n",
        "**Entropy measures of uncertainty** \n",
        "\n",
        "* MEAN Uncertainty = Entropy (expected prediction of ensemble) - measure of total uncertainty\n",
        "```\n",
        "def entropy_of_expected(probs):\n",
        "    [...]\n",
        "    return np.sum(mean_probs * log_probs, axis=-1)\n",
        "```\n",
        "* Expected Entropy = Mean (individual models entropies) - measure of data uncertainty\n",
        "```\n",
        "def expected_entropy(probs):\n",
        "  [...]\n",
        "  return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n",
        "```\n",
        "\n",
        "**Mutual Information**\n",
        "* We can fit a Gaussian distribution on top of the ensemble predictions on the simplex. The variance of the distribution is also indicative of model uncertainty. \n",
        "* The Mutual Information (MI) is computed between the probability distributions of each ensemble outcome. \n",
        "* Mutual information is computed as the difference between our total uncertainty estimate (entropy of expected) and our measure of data uncertainty (expected entropy) - measure of model uncertainty\n",
        "```\n",
        "def mutual_information(eoe, ee):\n",
        "  [...]\n",
        "  mutual_info = eoe - exe\n",
        "  return mutual_info\n",
        "```\n"
      ],
      "metadata": {
        "id": "tsf1eTpLHHiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Demonstration on White Matter Lesion Segmentation for MRI of Multiple Sclerosis "
      ],
      "metadata": {
        "id": "wTcP6poyCx-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [DEMO] Installation and set up"
      ],
      "metadata": {
        "id": "Ohvv6lw41u_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install torch\n",
        "!pip install monai[all]==0.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWGXXzrhwedS",
        "outputId": "ae2d196e-8988-411a-c4c7-f3a7d1e709fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Collecting monai[all]==0.3.0\n",
            "  Downloading monai-0.3.0-202010042353-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 25.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (0.18.3)\n",
            "Collecting pytorch-ignite==0.4.2\n",
            "  Downloading pytorch_ignite-0.4.2-py2.py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (2.8.0)\n",
            "Requirement already satisfied: gdown>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (4.4.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (4.64.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (3.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from monai[all]==0.3.0) (7.1.2)\n",
            "Collecting itk\n",
            "  Downloading itk-5.2.1.post1-cp37-cp37m-manylinux2014_x86_64.whl (8.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=3.6.4->monai[all]==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown>=3.6.4->monai[all]==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=3.6.4->monai[all]==0.3.0) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=3.6.4->monai[all]==0.3.0) (4.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->monai[all]==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->monai[all]==0.3.0) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->monai[all]==0.3.0) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->monai[all]==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->monai[all]==0.3.0) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14.2->monai[all]==0.3.0) (4.2.0)\n",
            "Collecting itk-registration==5.2.1.post1\n",
            "  Downloading itk_registration-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting itk-segmentation==5.2.1.post1\n",
            "  Downloading itk_segmentation-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6 MB 53.3 MB/s \n",
            "\u001b[?25hCollecting itk-filtering==5.2.1.post1\n",
            "  Downloading itk_filtering-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (95.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 95.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting itk-io==5.2.1.post1\n",
            "  Downloading itk_io-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.0 MB 62.8 MB/s \n",
            "\u001b[?25hCollecting itk-core==5.2.1.post1\n",
            "  Downloading itk_core-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (70.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 70.6 MB 8.2 kB/s \n",
            "\u001b[?25hCollecting itk-numerics==5.2.1.post1\n",
            "  Downloading itk_numerics-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.5 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.6.4->monai[all]==0.3.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.6.4->monai[all]==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.6.4->monai[all]==0.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.6.4->monai[all]==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.6.4->monai[all]==0.3.0) (1.7.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (1.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->monai[all]==0.3.0) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]==0.3.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]==0.3.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->monai[all]==0.3.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->monai[all]==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->monai[all]==0.3.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->monai[all]==0.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->monai[all]==0.3.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->monai[all]==0.3.0) (3.2.0)\n",
            "Installing collected packages: itk-core, itk-numerics, itk-filtering, itk-segmentation, itk-registration, itk-io, pytorch-ignite, monai, itk\n",
            "Successfully installed itk-5.2.1.post1 itk-core-5.2.1.post1 itk-filtering-5.2.1.post1 itk-io-5.2.1.post1 itk-numerics-5.2.1.post1 itk-registration-5.2.1.post1 itk-segmentation-5.2.1.post1 monai-0.3.0 pytorch-ignite-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained models from https://drive.google.com/file/d/1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU/view?usp=sharing\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU\" -O models.tar.gz && rm -rf /tmp/cookies.txt\n",
        "!tar -xzvf models.tar.gz"
      ],
      "metadata": {
        "id": "ZkDbyteB17iU",
        "outputId": "408fedf2-41ce-490e-93a4-51f5ea1a0106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-17 07:10:28--  https://docs.google.com/uc?export=download&confirm=t&id=1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.63.100, 172.253.63.138, 172.253.63.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.63.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-b4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ar1s8osr2b2eishtfmfh952ehvd6jb4/1652771400000/17412464615910489214/*/1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-17 07:10:29--  https://doc-04-b4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ar1s8osr2b2eishtfmfh952ehvd6jb4/1652771400000/17412464615910489214/*/1pPKKRSi1u0CK8NRkwQrfMBAV8q7onkHU?e=download\n",
            "Resolving doc-04-b4-docs.googleusercontent.com (doc-04-b4-docs.googleusercontent.com)... 172.253.63.132, 2607:f8b0:4004:c08::84\n",
            "Connecting to doc-04-b4-docs.googleusercontent.com (doc-04-b4-docs.googleusercontent.com)|172.253.63.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146396897 (140M) [application/x-gzip]\n",
            "Saving to: ‘models.tar.gz’\n",
            "\n",
            "models.tar.gz       100%[===================>] 139.61M  63.7MB/s    in 2.2s    \n",
            "\n",
            "2022-05-17 07:10:31 (63.7 MB/s) - ‘models.tar.gz’ saved [146396897/146396897]\n",
            "\n",
            "baseline/\n",
            "baseline/seed5/\n",
            "baseline/seed5/Best_model_finetuning.pth\n",
            "baseline/seed1/\n",
            "baseline/seed1/Best_model_finetuning.pth\n",
            "baseline/seed2/\n",
            "baseline/seed2/Best_model_finetuning.pth\n",
            "baseline/seed3/\n",
            "baseline/seed3/Best_model_finetuning.pth\n",
            "baseline/seed4/\n",
            "baseline/seed4/Best_model_finetuning.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load development data from https://drive.google.com/file/d/1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK/view?usp=sharing\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK\" -O data.tar.gz && rm -rf /tmp/cookies.txt\n",
        "!tar -xzvf data.tar.gz"
      ],
      "metadata": {
        "id": "HD6QVX7PG7UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b972499b-0a0a-411e-8e4e-55653b211d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-17 07:11:25--  https://docs.google.com/uc?export=download&confirm=t&id=1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.63.113, 172.253.63.139, 172.253.63.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.63.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-b4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q20ceihk23rkhadq9gnb80nvfqpapvq7/1652771475000/17412464615910489214/*/1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-17 07:11:25--  https://doc-0s-b4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q20ceihk23rkhadq9gnb80nvfqpapvq7/1652771475000/17412464615910489214/*/1fvyBzSCg7JTIDf4E3R4pCU-1hwL9zSoK?e=download\n",
            "Resolving doc-0s-b4-docs.googleusercontent.com (doc-0s-b4-docs.googleusercontent.com)... 172.253.63.132, 2607:f8b0:4004:c08::84\n",
            "Connecting to doc-0s-b4-docs.googleusercontent.com (doc-0s-b4-docs.googleusercontent.com)|172.253.63.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83558444 (80M) [application/x-gzip]\n",
            "Saving to: ‘data.tar.gz’\n",
            "\n",
            "data.tar.gz         100%[===================>]  79.69M  77.7MB/s    in 1.0s    \n",
            "\n",
            "2022-05-17 07:11:27 (77.7 MB/s) - ‘data.tar.gz’ saved [83558444/83558444]\n",
            "\n",
            "dev_in/\n",
            "dev_in/4_gt.nii\n",
            "dev_in/2_FLAIR.nii.gz\n",
            "dev_in/7_gt.nii\n",
            "dev_in/4_FLAIR.nii.gz\n",
            "dev_in/5_gt.nii\n",
            "dev_in/6_gt.nii\n",
            "dev_in/3_gt.nii\n",
            "dev_in/1_gt.nii\n",
            "dev_in/7_FLAIR.nii.gz\n",
            "dev_in/3_FLAIR.nii.gz\n",
            "dev_in/2_gt.nii\n",
            "dev_in/1_FLAIR.nii.gz\n",
            "dev_in/5_FLAIR.nii.gz\n",
            "dev_in/6_FLAIR.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from glob import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "#\n",
        "from monai.config import print_config\n",
        "from monai.data import CacheDataset, DataLoader, Dataset\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.losses import DiceLoss, GeneralizedDiceLoss, TverskyLoss\n",
        "from monai.metrics import compute_meandice, DiceMetric\n",
        "from monai.networks.layers import Norm\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    AddChanneld,Compose,CropForegroundd,LoadNiftid,Orientationd,RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,Spacingd,ToTensord,ConcatItemsd,NormalizeIntensityd, RandFlipd,\n",
        "    RandRotate90d,RandShiftIntensityd,RandAffined,RandSpatialCropd, AsDiscrete, Activations)\n",
        "from monai.utils import first, set_determinism\n",
        "from monai.data import write_nifti, create_file_basename, NiftiDataset"
      ],
      "metadata": {
        "id": "0YmnpwAJHOpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [DEMO] Preprocess the data "
      ],
      "metadata": {
        "id": "TjUOQ8bwHI86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_data = \"dev_in\"\n",
        "path_gts = \"dev_in\"\n",
        "flair = sorted(glob(os.path.join(path_data, \"*FLAIR.nii.gz\")),\n",
        "              key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n",
        "segs = sorted(glob(os.path.join(path_gts, \"*gt.nii\")),\n",
        "              key=lambda i: int(re.sub('\\D', '', i)))\n",
        "N = (len(flair)) # Number of subjects for training/validation, by default using all subjects in the folder\n",
        "indices = np.arange(N)\n",
        "v=indices[:]\n",
        "\n",
        "test_files=[]\n",
        "for j in v:\n",
        "    test_files = test_files + [{\"image\": fl, \"label\": seg} for fl, seg in zip(flair[j:j+1], segs[j:j+1])]\n",
        "\n",
        "print(\"Testing cases:\", len(test_files))\n",
        "val_transforms = Compose(\n",
        "[\n",
        "    LoadNiftid(keys=[\"image\", \"label\"]),\n",
        "    AddChanneld(keys=[\"image\",\"label\"]),\n",
        "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    NormalizeIntensityd(keys=[\"image\"], nonzero=True),\n",
        "    ToTensord(keys=[\"image\", \"label\"]),\n",
        "]\n",
        ")\n",
        "\n",
        "val_ds = CacheDataset(data=test_files, transform=val_transforms, cache_rate=0.5, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)"
      ],
      "metadata": {
        "id": "nzJ6SgavIFAY",
        "outputId": "f08249d3-f98d-4b8a-fbd2-a493dc8bad53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing cases: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Load and cache transformed data: 100%|██████████| 3/3 [00:15<00:00,  5.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [DEMO] Define functions to compute uncertainty measures"
      ],
      "metadata": {
        "id": "MRilKvlbKgxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implementation of standard predictive uncertainty measures for image segmentation\n",
        "\"\"\"\n",
        "\n",
        "def entropy_of_expected(probs, epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
        "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
        "    \"\"\"\n",
        "    mean_probs = np.mean(probs, axis=0)\n",
        "    log_probs = -np.log(mean_probs + epsilon)\n",
        "    return np.sum(mean_probs * log_probs, axis=-1)\n",
        "\n",
        "def expected_entropy(probs, epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
        "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
        "    \"\"\"\n",
        "    log_probs = -np.log(probs + epsilon)\n",
        "    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n",
        "\n",
        "\n",
        "def ensemble_uncertainties_classification(probs, epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
        "    :return: Dictionary of uncertainties\n",
        "    \"\"\"\n",
        "    mean_probs = np.mean(probs, axis=0)\n",
        "    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n",
        "    conf = np.max(mean_probs, axis=-1)\n",
        "\n",
        "    eoe = entropy_of_expected(probs, epsilon)\n",
        "    exe = expected_entropy(probs, epsilon)\n",
        "\n",
        "    mutual_info = eoe - exe\n",
        "\n",
        "    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n",
        "\n",
        "    uncertainty = {'confidence': -1 * conf,\n",
        "                   'entropy_of_expected': eoe,\n",
        "                   'expected_entropy': exe,\n",
        "                   'mutual_information': mutual_info,\n",
        "                   'epkl': epkl,\n",
        "                   'reverse_mutual_information': epkl - mutual_info\n",
        "                   }\n",
        "\n",
        "    return uncertainty"
      ],
      "metadata": {
        "id": "OuHfcCAFKsxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [DEMO] Run inference and compute estimates over ensemble of 5 models\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zX2b0zyHIk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy import ndimage\n",
        "\n",
        "# Threshold to convert probabilistic prediction into binary segmentation mask\n",
        "th = 0.35\n",
        "# Set number of models in ensemble\n",
        "K = 5\n",
        "root_dir = \"baseline/\"\n",
        "\n",
        "\n",
        "# Set device\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Got CUDA!\")\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n",
        "\n",
        "models = []\n",
        "for i in range(K):\n",
        "    models.append(UNet(dimensions=3,in_channels=1, out_channels=2,channels=(32, 64, 128, 256, 512),\n",
        "                strides=(2, 2, 2, 2),num_res_units=0).to(device))\n",
        "  \n",
        "act = Activations(softmax=True)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(root_dir + \"seed\" + str(i+1) + \"/Best_model_finetuning.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "all_inputs = []\n",
        "all_predictions = []\n",
        "all_groundTruths = []\n",
        "all_uncs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for patient_count, batch_data in enumerate(val_loader):\n",
        "        print(\"Patient num: \", patient_count)\n",
        "        inputs, gt  = (\n",
        "                batch_data[\"image\"].to(device),#.unsqueeze(0),\n",
        "                  batch_data[\"label\"].type(torch.LongTensor).to(device),)#.unsqueeze(0),)\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 4\n",
        "\n",
        "        all_outputs = []\n",
        "        for model in models:\n",
        "            outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, model, mode='gaussian')\n",
        "            outputs_o = (act(outputs))\n",
        "            outputs = act(outputs).cpu().numpy()\n",
        "            outputs = np.squeeze(outputs[0,1])\n",
        "            all_outputs.append(outputs)\n",
        "        all_outputs = np.asarray(all_outputs)\n",
        "        outputs = np.mean(all_outputs, axis=0)\n",
        "\n",
        "        # Get all uncertainties\n",
        "        uncs = ensemble_uncertainties_classification( np.concatenate( (np.expand_dims(all_outputs, axis=-1), np.expand_dims(1.-all_outputs, axis=-1)), axis=-1) )\n",
        "        \n",
        "        outputs[outputs>th]=1\n",
        "        outputs[outputs<th]=0\n",
        "        seg= np.squeeze(outputs)\n",
        "\n",
        "        val_labels = gt.cpu().numpy()\n",
        "        gt = np.squeeze(val_labels)\n",
        "\n",
        "        \"\"\"\n",
        "        Remove connected components smaller than 10 voxels\n",
        "        \"\"\"\n",
        "        l_min = 9\n",
        "        labeled_seg, num_labels = ndimage.label(seg)\n",
        "        label_list = np.unique(labeled_seg)\n",
        "        num_elements_by_lesion = ndimage.labeled_comprehension(seg,labeled_seg,label_list,np.sum,float, 0)\n",
        "\n",
        "        seg2 = np.zeros_like(seg)\n",
        "        for l in range(len(num_elements_by_lesion)):\n",
        "            if num_elements_by_lesion[l] > l_min:\n",
        "        # assign voxels to output\n",
        "                current_voxels = np.stack(np.where(labeled_seg == l), axis=1)\n",
        "                seg2[current_voxels[:, 0],\n",
        "                    current_voxels[:, 1],\n",
        "                    current_voxels[:, 2]] = 1\n",
        "        seg=np.copy(seg2)\n",
        "\n",
        "        # Get the Dice score\n",
        "        im_sum = np.sum(seg) + np.sum(gt)\n",
        "        if im_sum == 0:\n",
        "            value = 1.0\n",
        "            dsc = value\n",
        "        else:\n",
        "            value = (np.sum(seg[gt==1])*2.0) / (np.sum(seg) + np.sum(gt))\n",
        "            dsc = value.sum().item()\n",
        "        print(\"Dice score:\", dsc)\n",
        "\n",
        "        all_predictions.append(seg)\n",
        "        all_groundTruths.append(gt)\n",
        "        all_uncs.append(uncs)\n",
        "        all_inputs.append(np.squeeze(inputs.cpu().numpy()))\n",
        "\n",
        "        # For the sake of the investigation of interpretability, we only look at the results for the first patient\n",
        "        break"
      ],
      "metadata": {
        "id": "_o5LJqKSr7NW",
        "outputId": "86e8e6a5-86ef-40f5-e105-aa845827cc26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient num:  0\n",
            "Dice score: 0.8380670829419241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [DEMO] Visualise predictions and uncertainty maps"
      ],
      "metadata": {
        "id": "v6xPA24bN3R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_theme()"
      ],
      "metadata": {
        "id": "9rOflFtysfJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select our choice of uncertainty measure\n",
        "unc_measure = \"entropy_of_expected\"\n",
        "# Plot the first ground truth and corresponding prediction at a random slice\n",
        "gt, pred, unc, inp = all_groundTruths[0], all_predictions[0], all_uncs[0][unc_measure], all_inputs[0]\n",
        "slice_num=100\n",
        "# eliminate uncertainty scores outside brain region\n",
        "unc[inp==0] = 0\n",
        "gt_slice, pred_slice, unc_slice, inp_slice = np.rot90(gt[slice_num,:,:], 1), np.rot90(pred[slice_num,:,:], 1), np.rot90(unc[slice_num,:,:], 1), np.rot90(inp[slice_num,:,:], 1)\n",
        "\n",
        "\n",
        "ax = sns.heatmap(inp_slice, cbar=False, cmap=\"Greys\", xticklabels=False, yticklabels=False)\n",
        "plt.show()\n",
        "plt.title(\"2D-Slice MRI - FLAIR\")\n",
        "plt.clf()\n",
        "\n",
        "ax = sns.heatmap(gt_slice, cbar=False, xticklabels=False, yticklabels=False)\n",
        "plt.show()\n",
        "plt.title(\"2D-Slice Lesion Annotation\")\n",
        "plt.clf()\n",
        "\n",
        "sns.heatmap(pred_slice, cbar=False, xticklabels=False, yticklabels=False)\n",
        "plt.show()\n",
        "plt.title(\"2D-Slice Lesion Prediction of Ensemble\")\n",
        "plt.clf()\n",
        "\n",
        "sns.heatmap(unc_slice, cbar=False, xticklabels=False, yticklabels=False)\n",
        "plt.show()\n",
        "lt.title(\"2D-Slice Total Uncertainty (Entropy of Expected)\")\n",
        "plt.clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "d95GWa1AN5H7",
        "outputId": "ed94a39c-c0ae-4092-df66-5e19a156453f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aa39ba0ad1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munc_measure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"entropy_of_expected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the first ground truth and corresponding prediction at a random slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_groundTruths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_uncs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munc_measure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mslice_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# eliminate uncertainty scores outside brain region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_groundTruths' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nos0B4QYsW_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}